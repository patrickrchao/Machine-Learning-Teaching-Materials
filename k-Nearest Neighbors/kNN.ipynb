{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: Classification with k-Nearest Neighbors\n",
    "## Patrick Chao 1/21/18\n",
    "### (Adapted from Data 8 Fall 2017 Project 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The tutorial is centered around the third project from Data 8 Fall 2017. This project involves classifying a movie's genre as either action or romance using k-Nearest Neighbors.\n",
    "\n",
    "# Classification vs Regression\n",
    "In general, there are two major types of machine learning problems, classification and regression.\n",
    "\n",
    "*Classification* is a problem where we would like to *classify* some sample input into a class or category. For example, we could classify the genre of a movie or classify a handwritten digit as $0-9$. The input may be a list of **features**, or *qualities* of a sample (for digits this would the individual pixels of the image), and the output is a class or label. Note that the bins are discrete and often categorical, and there are a finite number of classes. \n",
    "\n",
    "For modeling classification problems, this may involve generating a probability for each class, and selecting the class with the highest probability. In this notebook, we will investigate a simpler model, using a method called *k-nearest neighbors*.\n",
    "\n",
    "*Regression* does not depend on distinct classes for labels. The input is still a set of features, but the output is instead a continuous value. This may be predicting the population in $5$ years or the temperature tomorrow. In this situation, the \"right answer\" is more vague. If we predict the temperature to be $70$ degrees tomorrow but it is actually $71$, are we right? What if we predicted $70.5$ degrees? This adds a layer of complexity between regression and classification. \n",
    "\n",
    "For modeling regression problems this may be done by creating some function approximation in terms of the input. For example, linear regression is the simplest model, and outputs a continuous value. There are more complex models such as *neural networks* that act as universal function approximators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "The **k-Nearest Neighbors** (kNN) algorithm is one of the simplest models. The core idea is that a similar set of features should have the same label. For example if we receive an image $A$ as input where we would like to classify the digit, we could look at what other images look like $A$ in our training set. If we were doing $5$-nearest neighbors, we would find the $5$ images closest to $A$ in our data set, and return the most common digit among the $5$. In general, you may choose any value for $k$, $5$ may not be the best choice. Note, this has a $O(1)$ training time! This is the fastest algorithm for training, as there is no training!\n",
    "\n",
    "However, some questions immediately arise. How do you determine how close two images are? Why choose $5$, not $10$ or $100$? There are other consequences as well; you need to look through your entire dataset each time to determine the $k$ closest images, which could take a long time if your training set is huge. The prediction time for kNN is $O(n)$, which is much slower than something like linear regression, where the prediction is $O(d)$ where $d$ is the number of dimensions of the input data.\n",
    "\n",
    "We will address these questions and the shortcomings of kNN.\n",
    "\n",
    "A few conceptual questions for understanding:\n",
    "1. In binary classification (two classes), why is choosing an odd value for $k$ better than an even value?\n",
    "2. Given two separate ordered pairs of two values, $(a,b)$ and $(x,y)$, what possibilities are there for calculating the \"distance\" between them? What are the differences between approaches?\n",
    "3. Assume we are doing image classification. List any possible issues with image classification.\n",
    "4. What does 1-NN mean? If we have $n$ training data, what is $n$-NN? What are some of the *tradeoffs* for varying $k$ between $1$ and $n$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "#You may need to pip install pandas/matplotlib\n",
    "\n",
    "#Given a movie title, this returns the frequency of given words\n",
    "def wordFreq(movieTitle,words):\n",
    "    #Change movieTitle to lower case\n",
    "    movieTitle=movieTitle.lower()\n",
    "    \n",
    "    #Change words to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    #Check if movie title is found\n",
    "    try:\n",
    "        movie = movies[movies[\"Title\"]==movieTitle]\n",
    "    except:\n",
    "        print(\"Movie title not found!\")\n",
    "        return \"\"\n",
    "    \n",
    "    #Check if given words are not found\n",
    "    try:\n",
    "        wordFrequencies = movie[words].as_matrix()[0]\n",
    "    except:\n",
    "        print(\"Words not found\")\n",
    "        return \"\"\n",
    "    \n",
    "    return wordFrequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rating</th>\n",
       "      <th># Votes</th>\n",
       "      <th># Words</th>\n",
       "      <th>i</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>foster</th>\n",
       "      <th>pub</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>garrison</th>\n",
       "      <th>grammoo</th>\n",
       "      <th>chimney</th>\n",
       "      <th>bikini</th>\n",
       "      <th>richter</th>\n",
       "      <th>psychopath</th>\n",
       "      <th>fling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the terminator</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>8.1</td>\n",
       "      <td>183538</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.040022</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.024878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batman</td>\n",
       "      <td>action</td>\n",
       "      <td>1989</td>\n",
       "      <td>7.6</td>\n",
       "      <td>112731</td>\n",
       "      <td>2836</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.033850</td>\n",
       "      <td>0.023977</td>\n",
       "      <td>0.028209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tomorrow never dies</td>\n",
       "      <td>action</td>\n",
       "      <td>1997</td>\n",
       "      <td>6.4</td>\n",
       "      <td>47198</td>\n",
       "      <td>4215</td>\n",
       "      <td>0.028707</td>\n",
       "      <td>0.054330</td>\n",
       "      <td>0.030368</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batman forever</td>\n",
       "      <td>action</td>\n",
       "      <td>1995</td>\n",
       "      <td>5.4</td>\n",
       "      <td>77223</td>\n",
       "      <td>3032</td>\n",
       "      <td>0.036609</td>\n",
       "      <td>0.042216</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>supergirl</td>\n",
       "      <td>action</td>\n",
       "      <td>1984</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6576</td>\n",
       "      <td>3842</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.032275</td>\n",
       "      <td>0.028891</td>\n",
       "      <td>0.026288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title   Genre  Year  Rating  # Votes  # Words         i  \\\n",
       "0       the terminator  action  1984     8.1   183538     1849  0.040022   \n",
       "1               batman  action  1989     7.6   112731     2836  0.051481   \n",
       "2  tomorrow never dies  action  1997     6.4    47198     4215  0.028707   \n",
       "3       batman forever  action  1995     5.4    77223     3032  0.036609   \n",
       "4            supergirl  action  1984     4.1     6576     3842  0.041905   \n",
       "\n",
       "        the        to         a  ...    foster  pub  vegetarian  garrison  \\\n",
       "0  0.043807  0.025419  0.024878  ...       0.0  0.0         0.0       0.0   \n",
       "1  0.033850  0.023977  0.028209  ...       0.0  0.0         0.0       0.0   \n",
       "2  0.054330  0.030368  0.021827  ...       0.0  0.0         0.0       0.0   \n",
       "3  0.042216  0.020449  0.031003  ...       0.0  0.0         0.0       0.0   \n",
       "4  0.032275  0.028891  0.026288  ...       0.0  0.0         0.0       0.0   \n",
       "\n",
       "   grammoo  chimney  bikini  richter  psychopath  fling  \n",
       "0      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "1      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "2      0.0      0.0     0.0      0.0    0.000237    0.0  \n",
       "3      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "4      0.0      0.0     0.0      0.0    0.000000    0.0  \n",
       "\n",
       "[5 rows x 5006 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see what our dataset looks like!\n",
    "movies = pd.read_csv('movies.csv')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "[0.00035261 0.05148096]\n"
     ]
    }
   ],
   "source": [
    "#What type is movies?\n",
    "print(type(movies))\n",
    "\n",
    "#What is the frequency of the words \"hey\" and \"i\" in the matrix? Try some yourself! \n",
    "print(wordFreq(\"batman\",[\"Hey\",\"i\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification and Feature Selection\n",
    "Our goal is to be able to classify songs based on the frequency of various words in the script. However it is not feasible to use all the words as that is very calculation intensive. An alternative is to select certain features, but what features do we select? One method to look at which words are often in romance movies but not action, and vice versa. This is called **feature selection**. \n",
    "\n",
    "First, we will separate the data into training and validation data. Next, we may create some elementary functions such as the distance between movies, getting movies as pandas series, and finding the $k$ movies closest to some given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data into 80 training and 20 validation\n",
    "trainingPercentage = 80/100\n",
    "numMovies = movies.shape[0]\n",
    "numTraining = (int)(numMovies*trainingPercentage)\n",
    "numValidation = numMovies - numTraining\n",
    "\n",
    "#Training Set\n",
    "trainingSet = movies[0:numTraining]\n",
    "\n",
    "#Validation Set\n",
    "validationSet = movies[numTraining:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate into action and romance\n",
    "action = trainingSet[trainingSet[\"Genre\"]==\"action\"]\n",
    "romance = trainingSet[trainingSet[\"Genre\"]==\"romance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given two movie titles mov1,mov2, and a list of words\n",
    "#distance returns the euclidean distance between the two movies using the words as features\n",
    "def distance(mov1,mov2,words):\n",
    "    mov1Freq=wordFreq(mov1,words)\n",
    "    mov2Freq=wordFreq(mov2,words)\n",
    "    return np.sqrt(sum((mov1Freq-mov2Freq)**2))\n",
    "\n",
    "#Given a movie title, this returns the row as a pandas series\n",
    "def getMovie(title):\n",
    "    title = title.lower()\n",
    "    return movies[movies[\"Title\"]==title].squeeze()\n",
    "\n",
    "#Given a movie as a panda series, determines the k closest movies using words as features\n",
    "#Returns the dataframe of movies\n",
    "def kShortestDistance(k,movie,movieSet,words):\n",
    "    distances=[]\n",
    "    #Iterate over all movies\n",
    "    for i in range(movieSet.shape[0]):\n",
    "        currMovieTitle = movieSet.iloc[i][\"Title\"]\n",
    "        #Get the distance of two movies from two movies \n",
    "        dist = distance(currMovieTitle,movie[\"Title\"],words)\n",
    "        distances.append((dist,i))\n",
    "    #Sort the array\n",
    "    distances = sorted(distances,key=lambda x:x[0])\n",
    "    #Get the indices of the movies\n",
    "    indices = [x[1] for x in distances]\n",
    "    return movieSet.iloc[indices[1:k+1]]\n",
    "\n",
    "#Faster kShortestDistance using subsetting\n",
    "def kShortestDistanceFast(k,movie,movieSet,words):\n",
    "    #Subset out the words\n",
    "    movieSubset = movieSet[words]\n",
    "    currMovie = movie[words].squeeze()\n",
    "    #Calculate Distances and sort\n",
    "    distances = ((movieSubset-currMovie)**2).sum(axis=1)\n",
    "    distances = distances.sort_values()\n",
    "    #Shift by the minimum index if the movies do not start at 0\n",
    "    indices = distances.index.tolist()\n",
    "    minIndex = min(indices)\n",
    "    shiftedIndices=(np.array(indices)-minIndex).tolist()\n",
    "    return movieSet.iloc[shiftedIndices[1:k+1]]\n",
    "\n",
    "#Given a list of movies, returns the majority genre\n",
    "def getMajority(nearestMovies):\n",
    "    numMovies = nearestMovies.shape[0]\n",
    "    #Count frequency of genres\n",
    "    counts = nearestMovies['Genre'].value_counts()\n",
    "    if len(counts)==1:\n",
    "        return [x[0] for x in counts.items()][0]\n",
    "    if counts[\"action\"]>numMovies/2:\n",
    "        return \"action\"\n",
    "    return \"romance\"\n",
    "\n",
    "#Given a dataset, a set of word features, and the value of k\n",
    "#Returns the percentage correct (0-100)\n",
    "def accuracy(dataset,features,k):\n",
    "    numCorrect = 0 \n",
    "    #Iterate over all movies\n",
    "    for i in range(dataset.shape[0]):\n",
    "        currMovie = dataset.iloc[i].squeeze()\n",
    "        currMovieGenre = currMovie[\"Genre\"]\n",
    "        #Calculate k closest movies\n",
    "        kClosest = kShortestDistanceFast(k,currMovie,dataset,features)\n",
    "        predGenre = getMajority(kClosest)\n",
    "        #Keep track of number of correct predictions\n",
    "        if predGenre == currMovieGenre:\n",
    "            numCorrect +=1\n",
    "    #Return accuracy as percentage\n",
    "    return numCorrect*1.0/dataset.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses \"power\" and \"love\" as features to find the $5$ closest movies to \"batman returns\". Then we get the majority of the genres of those five movies, and we find that batman returns is predicted to be action based on those $5$ movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'action'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use \"money\" and \"feel\" as features\n",
    "features = [\"power\",\"love\"]\n",
    "movie = getMovie(\"batman returns\")\n",
    "#Get the five closest movies to the \"batman returns\" using the training set\n",
    "closest=kShortestDistance(5,movie,trainingSet,features)\n",
    "#Given the closest movies, returns the majority \n",
    "#Represents the kNN Prediction\n",
    "getMajority(closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this word plot (courtesy of Data 8) to construct some of your own features!\n",
    "<img src='wordplot.png' width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.17616580310881"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try with some of your own features!\n",
    "features = [\"power\",\"feel\"]\n",
    "k=5\n",
    "accuracy(trainingSet,features,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Our chosen features\n",
    "features = [\"men\",\"power\",\"marri\",\"nice\",\"home\",\"captain\",\"move\",\"run\",\"world\",\"huh\",\"happi\",\"move\",\"write\",\"hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our own chosen features, we then use the training set to determine the optimal value for $k$. Afterwards, we use this value of $k$ to find the accuracy on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0EVEEoQKXUQkQajQRUQShApdRCRB/D/ZYiESWExieAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a6ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Determine the best value for k\n",
    "trainAccuracies = []\n",
    "numKValues = 10\n",
    "for i in range(numKValues):\n",
    "    acc =accuracy(trainingSet,features,2*i+1)\n",
    "    trainAccuracies.append(acc)\n",
    "xAxis = ([2*i+1 for i in range(numKValues)])\n",
    "plt.plot(xAxis,trainAccuracies)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous information about overfitting and underfitting, explain the shape of the graph! Why is the accuracy low for $k=1$ and as $k$ increases past $15$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 11\n",
      "Test Accuracy: 71.42857142857143\n"
     ]
    }
   ],
   "source": [
    "#Determine best value for k\n",
    "optimalK=xAxis[np.argmax(trainAccuracies)]\n",
    "\n",
    "#Best kNN was found with k=11\n",
    "print(\"Best k:\",optimalK)\n",
    "\n",
    "#Determine validation error with this value for k\n",
    "optimalKNNVal = accuracy(validationSet,features,optimalK)\n",
    "print(\"Test Accuracy:\",optimalKNNVal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the accuracy for the validation set lower than the training accuracy (about $75\\%$)?\n",
    "\n",
    "This concludes the kNN python tutorial. k-Nearest Neighbors is an incredibly simple yet powerful model, and serves as a great foundational method for machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
